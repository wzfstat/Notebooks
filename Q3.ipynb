{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.5/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from sklearn.metrics import roc_auc_score, confusion_matrix, average_precision_score\n",
    "from sklearn.preprocessing import StandardScaler, Imputer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV, cross_val_score\n",
    "from time import time\n",
    "import itertools, gc, operator, os, pickle\n",
    "import multiprocessing as mp\n",
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "label_name = 'mt_hh'\n",
    "os.system('mkdir '+label_name)\n",
    "\n",
    "paths = ['../../../data/pnc/inbound/clv_hhsamp'+n+'.csv' for n in list('1234')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting eligible data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def sel_sample(path, feature, values, chunksize, usecols=None):\n",
    "    reader = pd.read_csv(path, chunksize=chunksize, usecols=usecols)\n",
    "    data = None\n",
    "    start = time()\n",
    "    for j, chunk in enumerate(reader):\n",
    "        items = chunk.loc[chunk[feature].isin(values)]\n",
    "        data = pd.concat([data, items])\n",
    "        del items\n",
    "        if j%10 == 9:\n",
    "            print('{} seconds: completed {} rows'.format(round(time() - start,2), (j+1)*chunksize))\n",
    "    gc.collect()\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_y_q3(paths, label_name, sel_by, sv_key, sv_label):\n",
    "    start = time()\n",
    "\n",
    "    parallel = mp.Pool(processes=len(paths)) \n",
    "    returner = [parallel.apply_async(sel_sample, args=(paths[i],), \n",
    "                kwds={'feature':sel_by, 'values':sv_label+[sv_key], 'chunksize':100000, 'usecols':['rlb_location_key', label_name, sel_by]})\n",
    "                for i in range(len(paths))]\n",
    "\n",
    "    print('Begin extracting data ...')\n",
    "    items = pd.concat([p.get() for p in returner])\n",
    "    parallel.close()\n",
    "    parallel.join()\n",
    "    del parallel\n",
    "    gc.collect()\n",
    "    \n",
    "    keys = items.loc[(items[label_name]==0) & (items[sel_by]==sv_key), ['rlb_location_key']]\n",
    "    data_y = items.groupby('rlb_location_key')[label_name].max().reset_index()\n",
    "    data_y = pd.merge(keys, data_y, on='rlb_location_key', how='left')\n",
    "    print('----------')\n",
    "    print('The overall processing time is {} seconds.'.format(round(time() - start,2)))\n",
    "    del keys, items\n",
    "    return data_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_X_q3(paths, label_name, feature, values):\n",
    "    file_paths = []\n",
    "    for i, path in enumerate(paths):\n",
    "        print('====================')\n",
    "        filename = path[-1*path[::-1].find('/'):] if path.find('/') != -1 else path\n",
    "        print('Begin processing data from '+filename+' ......')\n",
    "        items = sel_sample(path, feature, values, chunksize=100000, usecols=None)\n",
    "        print('----- Done! Begin selecting eligible samples ...')\n",
    "        group = items.groupby('rlb_location_key')[label_name].count().reset_index()\n",
    "        keys = group.loc[group[label_name]==len(values), 'rlb_location_key'].values\n",
    "        items = items.loc[items.rlb_location_key.isin(keys)].sort_values(['rlb_location_key',feature]).reset_index(drop=True)\n",
    "        print('----- Done!')\n",
    "        save_path = label_name+'/data_q3_X_'+str(i+1)+'.csv'\n",
    "        items.to_csv(save_path, index=False)\n",
    "        print('The file has been saved to '+save_path)\n",
    "        file_paths.append(save_path)\n",
    "        del items\n",
    "        gc.collect()\n",
    "    print(str(len(paths))+' files have been save!')\n",
    "    return file_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Begin extracting data ...\n",
      "26.85 seconds: completed 1000000 rows\n",
      "26.91 seconds: completed 1000000 rows\n",
      "27.13 seconds: completed 1000000 rows\n",
      "27.27 seconds: completed 1000000 rows\n",
      "55.61 seconds: completed 2000000 rows\n",
      "55.68 seconds: completed 2000000 rows\n",
      "55.81 seconds: completed 2000000 rows\n",
      "57.28 seconds: completed 2000000 rows\n",
      "84.1 seconds: completed 3000000 rows\n",
      "84.24 seconds: completed 3000000 rows\n",
      "84.72 seconds: completed 3000000 rows\n",
      "85.97 seconds: completed 3000000 rows\n",
      "110.16 seconds: completed 4000000 rows\n",
      "110.17 seconds: completed 4000000 rows\n",
      "110.73 seconds: completed 4000000 rows\n",
      "112.67 seconds: completed 4000000 rows\n",
      "141.86 seconds: completed 5000000 rows\n",
      "141.92 seconds: completed 5000000 rows\n",
      "142.26 seconds: completed 5000000 rows\n",
      "143.2 seconds: completed 5000000 rows\n",
      "160.23 seconds: completed 6000000 rows\n",
      "160.26 seconds: completed 6000000 rows\n",
      "160.32 seconds: completed 6000000 rows\n",
      "160.46 seconds: completed 6000000 rows\n",
      "----------\n",
      "The overall processing time is 161.81 seconds.\n"
     ]
    }
   ],
   "source": [
    "data_y = get_y_q3(paths, label_name, 'time_period', sv_key='2016-10-31', \n",
    "                  sv_label=['2016-11-30','2016-12-31','2017-01-31','2017-02-28','2017-03-31','2017-04-30'])\n",
    "data_y.to_csv(label_name+'/data_q3_y.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "file_paths = get_X_q3(paths, label_name, 'time_period',  \n",
    "                      ['2015-09-30','2015-10-31','2015-11-30','2015-12-31','2016-01-31','2016-02-29',\n",
    "                       '2016-03-31','2016-04-30','2016-05-31','2016-06-30','2016-07-31','2016-08-31'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================\n",
      "Begin processing data from clv_hhsamp1.csv ......\n",
      "41.19 seconds: completed 1000000 rows\n",
      "90.07 seconds: completed 2000000 rows\n",
      "146.42 seconds: completed 3000000 rows\n",
      "213.85 seconds: completed 4000000 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.5/site-packages/IPython/core/interactiveshell.py:2881: DtypeWarning: Columns (13) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "287.91 seconds: completed 5000000 rows\n",
      "370.48 seconds: completed 6000000 rows\n",
      "----- Done! Begin selecting eligible samples ...\n",
      "----- Done!\n",
      "The file has been saved to mt_hh/data_q3_X_1.csv\n",
      "====================\n",
      "Begin processing data from clv_hhsamp2.csv ......\n",
      "42.72 seconds: completed 1000000 rows\n",
      "100.94 seconds: completed 2000000 rows\n",
      "165.55 seconds: completed 3000000 rows\n",
      "234.72 seconds: completed 4000000 rows\n",
      "312.96 seconds: completed 5000000 rows\n",
      "402.24 seconds: completed 6000000 rows\n",
      "----- Done! Begin selecting eligible samples ...\n",
      "----- Done!\n",
      "The file has been saved to mt_hh/data_q3_X_2.csv\n",
      "====================\n",
      "Begin processing data from clv_hhsamp3.csv ......\n",
      "47.67 seconds: completed 1000000 rows\n",
      "117.74 seconds: completed 2000000 rows\n",
      "182.63 seconds: completed 3000000 rows\n",
      "260.62 seconds: completed 4000000 rows\n",
      "348.31 seconds: completed 5000000 rows\n",
      "451.64 seconds: completed 6000000 rows\n",
      "----- Done! Begin selecting eligible samples ...\n",
      "----- Done!\n",
      "The file has been saved to mt_hh/data_q3_X_3.csv\n",
      "====================\n",
      "Begin processing data from clv_hhsamp4.csv ......\n",
      "49.09 seconds: completed 1000000 rows\n",
      "102.73 seconds: completed 2000000 rows\n",
      "165.95 seconds: completed 3000000 rows\n",
      "252.13 seconds: completed 4000000 rows\n",
      "344.65 seconds: completed 5000000 rows\n",
      "455.8 seconds: completed 6000000 rows\n",
      "----- Done! Begin selecting eligible samples ...\n",
      "----- Done!\n",
      "The file has been saved to mt_hh/data_q3_X_4.csv\n",
      "4 files have been save!\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.5/site-packages/IPython/core/interactiveshell.py:2717: DtypeWarning: Columns (13) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "data_y = pd.read_csv(label_name+'/data_q3_y.csv')\n",
    "file_paths = [label_name+'/data_q3_X_'+str(n+1)+'.csv' for n in range(4)]\n",
    "data_X = None\n",
    "for path in file_paths:\n",
    "    temp = pd.read_csv(path)\n",
    "    temp = pd.merge(temp.drop(label_name, axis=1), data_y, on='rlb_location_key', how='inner')\n",
    "    data_X = pd.concat([data_X, temp])\n",
    "    del temp\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "col_drop = [\n",
    "            'rel_tenure_src',\n",
    "            'dd_agr_type',\n",
    "            'sv_agr_type',\n",
    "            'mm_agr_type',\n",
    "            'cd_agr_type',\n",
    "            'cd_ira_agr_type',\n",
    "            'bk_agr_type',\n",
    "            'bk_ira_agr_type',\n",
    "            'ir_agr_type',\n",
    "            'pp_agr_type',\n",
    "            'cc_agr_type',\n",
    "            'mt_agr_type',\n",
    "            'heil_agr_type',\n",
    "            'heloc_agr_type',\n",
    "            'pil_agr_type',\n",
    "            'ploc_agr_type',\n",
    "            'auto_agr_type',\n",
    "            'sl_agr_type',\n",
    "            'iil_agr_type',\n",
    "            'sd_agr_type',\n",
    "            'in_agr_type',\n",
    "            'psycle_code_ne',\n",
    "            'sls_branch_book',\n",
    "            'market_book',\n",
    "            'zip_code',\n",
    "            'market_zip',\n",
    "            'rcb_consumer_hh',\n",
    "            'total_profit',\n",
    "            'total_profit_var',\n",
    "            'total_rev',\n",
    "            'total_net_int_inc',\n",
    "            'total_non_int_inc',\n",
    "            'total_exp',\n",
    "            'total_exp_fix',\n",
    "            'total_exp_var',\n",
    "            'total_exp_dis',\n",
    "            'mt_acct',\n",
    "            'mt_bal',\n",
    "            'mt_int',\n",
    "            'mt_rev',\n",
    "            'mt_exp',\n",
    "            'mt_tenure',\n",
    "            'mt_conv_hh',\n",
    "            'mt_start_bal',\n",
    "            'ixiwc_total_assets',\n",
    "            'ixiwc_inv',\n",
    "            'ixiwc_deposits',\n",
    "            'ixiwc_dd',\n",
    "            'ixiwc_sv',\n",
    "            'ixiwc_mm',\n",
    "            'ixiwc_cd',\n",
    "            'fico_auto',\n",
    "            'fico_cc',\n",
    "            'fico_heil',\n",
    "            'fico_heloc',\n",
    "            'fico_mt',\n",
    "            'date_opened_first_prod']\n",
    "\n",
    "data_X.drop(col_drop, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Garbage                    864775\n",
       "Philadelphia, PA           386080\n",
       "Pittsburgh, PA             317274\n",
       "New York, NY               287684\n",
       "Washington et al, DC-MD    218109\n",
       "Chicago, IL                206681\n",
       "Cleveland et al, OH        203142\n",
       "Detroit, MI                156328\n",
       "Cincinnati, OH             117054\n",
       "Indianapolis, IN           115728\n",
       "Baltimore, MD              114889\n",
       "Louisville, KY             111046\n",
       "Columbus, OH               106201\n",
       "Harrisburg et al, PA        76648\n",
       "Wilkes Barre et al, PA      74835\n",
       "W. Palm Beach et al, FL     69853\n",
       "Grand Rapids et al, MI      63105\n",
       "Raleigh et al, NC           62523\n",
       "Atlanta, GA                 55584\n",
       "Dayton, OH                  48997\n",
       "St. Louis, MO               46713\n",
       "Orlando et al, FL           46311\n",
       "Name: dma, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dma_entry = list(data_X.dma.value_counts().index[data_X.dma.value_counts()<=40000])\n",
    "data_X.loc[data_X.dma.isin(dma_entry),'dma'] = 'Garbage'\n",
    "data_X.dma.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "col_cat = ['inc_code_hh',\n",
    "           'hh_agr_type',\n",
    "           'core_agr_type',\n",
    "           'new_hh_traj_acq',\n",
    "           'new_hh_traj_seg',\n",
    "           'sales_channel',\n",
    "           'age_grp_4L',\n",
    "           'age_hh_src',\n",
    "           'inc_grp_3L',\n",
    "           'inc_code_hh_src',\n",
    "           'consumer_segment',\n",
    "           'lifestage',\n",
    "           'dma',\n",
    "           'market_clv']\n",
    "for f in col_cat:\n",
    "    if sum(data_X[f].isnull()) > 0:\n",
    "        data_X.loc[data_X[f].isnull(),f] = 'missing'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_X = pd.get_dummies(data_X, columns=col_cat, drop_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y = data_X.loc[:,['rlb_location_key', label_name]]\n",
    "data_X.drop(label_name, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((3761532, 328), (3761532, 2))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_X.shape, y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting training/test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_X.iloc[:2640000,:].to_csv(label_name+'/data_q3_X_trn.csv', index=False)\n",
    "data_X.iloc[2640000:,:].to_csv(label_name+'/data_q3_X_tst.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y.iloc[:2640000,:].to_csv(label_name+'/data_q3_y_trn.csv', index=False)\n",
    "y.iloc[2640000:,:].to_csv(label_name+'/data_q3_y_tst.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Reloading saved data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_trn = pd.read_csv(label_name+'/data_q3_X_trn.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_trn.drop(['rlb_location_key','time_period'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_tst = pd.read_csv(label_name+'/data_q3_X_tst.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_tst.drop(['rlb_location_key','time_period'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1121532, 328)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_tst.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2640000, 326)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_trn.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Impute the missing using the most frequent value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "imps = dict()\n",
    "cols = list(X_trn.columns)\n",
    "for col in cols:\n",
    "    imp_value = X_trn[col].value_counts().index[0]\n",
    "    imps[col] = imp_value\n",
    "    X_trn.loc[:,col] = X_trn[col].fillna(imp_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Standardize the training part (not implemented here, just extract the means and standard deviations, will be implemented in training process)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scalers = dict()\n",
    "cols = list(X_trn.columns)\n",
    "for col in cols:\n",
    "    avg = X_trn[col].mean()\n",
    "    std = X_trn[col].std()\n",
    "    scalers[col] = (avg, std)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Store the imputing and standardization information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('imps', 'wb') as fp:\n",
    "    pickle.dump(imps, fp)\n",
    "with open('scalers', 'wb') as fp:\n",
    "    pickle.dump(scalers, fp) \n",
    "with open('features', 'wb') as fp:\n",
    "    pickle.dump(list(X_trn.columns), fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.save('X_trn_3d.npy', X_trn_3d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "imps = pickle.load(open('imps', \"rb\"))\n",
    "scalers = pickle.load(open('scalers', \"rb\"))\n",
    "features = pickle.load(open('features', \"rb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the stored imputing information to impute missings in test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for k, v in imps.items():\n",
    "    X_tst.loc[:,k] = X_tst[k].fillna(v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reshape the data to a 3D format. (household x months x features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.5/site-packages/ipykernel/__main__.py:1: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n",
      "  if __name__ == '__main__':\n"
     ]
    }
   ],
   "source": [
    "X_trn_3d = X_trn.as_matrix().reshape(X_trn.shape[0]/12,12,-1)\n",
    "X_tst_3d = X_tst.as_matrix().reshape(X_tst.shape[0]/12,12,-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.save('X_trn_3d.npy', X_trn_3d)\n",
    "np.save('X_tst_3d.npy', X_tst_3d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the labels and store them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_trn = pd.read_csv(label_name+'/data_q3_y_trn.csv')\n",
    "y_trn = y_trn.groupby('rlb_location_key')[label_name].max().as_matrix()\n",
    "np.save('y_trn.npy', y_trn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y_tst = pd.read_csv(label_name+'/data_q3_y_tst.csv')\n",
    "y_tst = y_tst.groupby('rlb_location_key')[label_name].max().as_matrix()\n",
    "np.save('y_tst.npy', y_tst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(223, 110)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(y_trn), sum(y_tst)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Begin modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scalers = pickle.load(open('scalers', \"rb\"))\n",
    "features = pickle.load(open('features', \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_trn_3d = np.load('X_trn_3d.npy')\n",
    "y_trn = np.load('y_trn.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_tst_3d = np.load('X_tst_3d.npy')\n",
    "y_tst = np.load('y_tst.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(220000, 12, 326)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_trn_3d.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "means = [scalers[fea][0] for fea in features]\n",
    "stds = [scalers[fea][1] for fea in features]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Replace 0 standard deviations with 1. Then standardize the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "stds = list(map(lambda x:x if x!=0 else 1, stds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_tst_std = (X_tst_3d-means)/stds\n",
    "del X_tst_3d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A batch generator function for feeding into batches of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def batch_generator(batch_size=64):\n",
    "    means = [scalers[fea][0] for fea in features]\n",
    "    stds = [scalers[fea][1] for fea in features]\n",
    "    stds = list(map(lambda x:x if x!=0 else 1, stds))\n",
    "    index = list(range(X_trn_3d.shape[0]))\n",
    "    nb_batch = int(X_trn_3d.shape[0]/batch_size)\n",
    "    while True:\n",
    "        np.random.shuffle(index)\n",
    "        for i in range(nb_batch):\n",
    "            ind = index[i*batch_size:(i+1)*batch_size]\n",
    "            yield (X_trn_3d[ind]-means)/stds, y_trn[ind]\n",
    "#         if nb_batch*batch_size < X_trn_3d.shape[0]:\n",
    "#             ind = index[(nb_batch*batch_size):]\n",
    "#             yield (X_trn_3d[ind] - means)/stds, y_trn[ind]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A batch generator function with balanced batch samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def batch_generator_balanced(batch_size=64):\n",
    "    means = [scalers[fea][0] for fea in features]\n",
    "    stds = [scalers[fea][1] for fea in features]\n",
    "    stds = list(map(lambda x:x if x!=0 else 1, stds))\n",
    "    index = list(range(X_trn_3d.shape[0]))\n",
    "    pos_ind = list(np.where(y_trn == 1)[0])\n",
    "    neg_ind = list(set(index)-set(pos_ind))\n",
    "    nb_batch = int(len(neg_ind)/batch_size)\n",
    "    nb_batch_pos = int(len(pos_ind)/batch_size)\n",
    "    half_batch_size = int(batch_size/2)\n",
    "    while True:\n",
    "        np.random.shuffle(neg_ind)\n",
    "        for i in range(nb_batch):\n",
    "            neg_batch_ind = neg_ind[i*half_batch_size:(i+1)*half_batch_size]\n",
    "            j = i%nb_batch_pos\n",
    "            pos_batch_ind = pos_ind[j*half_batch_size:(j+1)*half_batch_size]\n",
    "            if j == nb_batch_pos-1:\n",
    "                np.random.shuffle(pos_ind)\n",
    "            yield (X_trn_3d[neg_batch_ind+pos_batch_ind]-means)/stds, y_trn[neg_batch_ind+pos_batch_ind]\n",
    "#         if nb_batch*batch_size < X_trn_3d.shape[0]:\n",
    "#             ind = index[(nb_batch*batch_size):]\n",
    "#             yield (X_trn_3d[ind] - means)/stds, y_trn[ind]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "os.environ['KERAS_BACKEND'] = 'tensorflow'\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Activation, SimpleRNN, Dense, Embedding, Convolution2D, MaxPooling2D, Flatten, Dropout, BatchNormalization\n",
    "from keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model=Sequential([\n",
    "        SimpleRNN(256, input_shape=(12, 326),\n",
    "                  activation='relu', recurrent_initializer=\"identity\"),\n",
    "        BatchNormalization(),\n",
    "        Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "model.compile(loss='binary_crossentropy', optimizer=Adam(), metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "3437/3437 [==============================] - 82s - loss: 0.0385 - acc: 0.9882    \n",
      "Epoch 2/5\n",
      "3437/3437 [==============================] - 82s - loss: 0.0201 - acc: 0.9948    \n",
      "Epoch 3/5\n",
      "3437/3437 [==============================] - 85s - loss: 0.0302 - acc: 0.9966    \n",
      "Epoch 4/5\n",
      "3437/3437 [==============================] - 84s - loss: 0.0621 - acc: 0.9940    \n",
      "Epoch 5/5\n",
      "3437/3437 [==============================] - 82s - loss: 0.0483 - acc: 0.9959    \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.50843531889905258"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb_sample = X_trn_3d.shape[0]\n",
    "model.fit_generator(batch_generator_balanced(), steps_per_epoch=int(nb_sample/64), epochs=5)#, class_weight={0:1,1:1000})\n",
    "pred = model.predict(X_tst_std)\n",
    "roc_auc_score(y_tst, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "3437/3437 [==============================] - 81s - loss: 0.0577 - acc: 0.9957    \n",
      "Epoch 2/3\n",
      "3437/3437 [==============================] - 85s - loss: 0.0600 - acc: 0.9956    \n",
      "Epoch 3/3\n",
      "3437/3437 [==============================] - 84s - loss: 0.0408 - acc: 0.9969    \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5187617895703508"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit_generator(batch_generator_balanced(), steps_per_epoch=int(nb_sample/64), epochs=3)\n",
    "pred = model.predict(X_tst_std)\n",
    "roc_auc_score(y_tst, pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_trn_3d = X_trn_3d.reshape(-1, 1, 12, 326)\n",
    "X_tst_std = X_tst_std.reshape(-1, 1, 12, 326)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((220000, 1, 12, 326), (93461, 1, 12, 326))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_trn_3d.shape, X_tst_std.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Convolution2D(\n",
    "    input_shape=(1, 12, 326),\n",
    "    filters=32,\n",
    "    kernel_size=(1,4),\n",
    "    strides=1,\n",
    "    padding='same',\n",
    "    data_format='channels_first',\n",
    "))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "\n",
    "model.add(MaxPooling2D(\n",
    "    pool_size=(1,2),\n",
    "    strides=2,\n",
    "    padding='same',\n",
    "    data_format='channels_first',\n",
    "))\n",
    "\n",
    "\n",
    "model.add(Convolution2D(64, (1,2), strides=1, padding='same', data_format='channels_first'))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(MaxPooling2D((1,2), 2, 'same', data_format='channels_first'))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(256))\n",
    "model.add(Activation('relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(1))\n",
    "model.add(Activation('sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.compile(Adam(), 'binary_crossentropy', ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "3437/3437 [==============================] - 2105s - loss: 0.0655 - acc: 0.9758  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.52594109621458018"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb_sample = X_trn_3d.shape[0]\n",
    "model.fit_generator(batch_generator_balanced(), steps_per_epoch=int(nb_sample/64), epochs=1)\n",
    "pred = model.predict(X_tst_std)\n",
    "roc_auc_score(y_tst, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "3017/3437 [=========================>....] - ETA: 310s - loss: 0.0101 - acc: 0.9970"
     ]
    }
   ],
   "source": [
    "model.fit_generator(batch_generator_balanced(), steps_per_epoch=int(nb_sample/64), epochs=1)\n",
    "pred = model.predict(X_tst_std)\n",
    "roc_auc_score(y_tst, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-r--r-- 1 pl74056 users 1205445583 Aug  4 02:50 mt_hh/data_q3_X_1.csv\r\n",
      "-rw-r--r-- 1 pl74056 users 1213549121 Aug  4 03:01 mt_hh/data_q3_X_2.csv\r\n",
      "-rw-r--r-- 1 pl74056 users 1214585416 Aug  4 03:12 mt_hh/data_q3_X_3.csv\r\n",
      "-rw-r--r-- 1 pl74056 users 1220695224 Aug  4 03:24 mt_hh/data_q3_X_4.csv\r\n",
      "-rw-r--r-- 1 pl74056 users 2339155925 Aug  8 20:05 mt_hh/data_q3_X_trn.csv\r\n",
      "-rw-r--r-- 1 pl74056 users  990931199 Aug  8 20:10 mt_hh/data_q3_X_tst.csv\r\n",
      "-rw-r--r-- 1 pl74056 users    7746817 Aug  8 04:52 mt_hh/data_q3_y.csv\r\n",
      "-rw-r--r-- 1 pl74056 users   58080023 Aug  8 20:06 mt_hh/data_q3_y_trn.csv\r\n",
      "-rw-r--r-- 1 pl74056 users   24673727 Aug  8 20:06 mt_hh/data_q3_y_tst.csv\r\n"
     ]
    }
   ],
   "source": [
    "! ls mt_hh/data_q3* -la"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
